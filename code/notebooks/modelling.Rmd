---
title: "Mercari EDA"
output: html_notebook
---

```{r}
library(tidyverse)
library(stringr)
```

Load train and create variables for different category levels.

```{r}
train = read_tsv("../../data/train.tsv") %>%
  mutate(category_name_1 = str_match(category_name, "(.*?)/.*")[,2],
         category_name_2 = str_match(category_name, "(.*)/.*")[,2]) %>%  
  rename(category_name_3 = category_name) %>% 
  mutate(target = log1p(price))
```

Just retain category_name_3 for which there are at least two instances.

```{r}
train_two = train %>% 
  group_by(category_name_3) %>% 
  summarise(n = n()) %>% 
  ungroup() %>% 
  filter(n >= 2) %>%
  left_join(train, by = "category_name_3") %>%  
  mutate(id = 1:nrow(.))
```

Split into train and validation, stratifying on category_name_3.

```{r}
train_train_ids = train_two %>% 
  group_by(category_name_3) %>% 
  sample_frac(0.5) %>% 
  `$`("id")
train_train = train_two %>% filter(id %in% train_train_ids)
train_val = train_two %>% filter(!(id %in% train_train_ids))
```

Try predicting price by category_name and item_condition_id.
Not enough memory to use category_name_3 so use category_name_2.
For now, drop category_name NA.

```{r}
# model_lm = train %>% 
#   filter(!is.na(category_name_3)) %>% 
#   lm(target ~ category_name_3 + item_condition_id, data = .) %>% 
#  broom::augment()
```

Needs 14.2 Gb, so fails.

Just average by category_name_3.

```{r}
category_means = train_train %>% 
  group_by(category_name_3) %>% 
  summarise(mean_target = mean(target))
category_means
```

Train set error.

```{r}
train_set_residuals = train_train %>% 
  left_join(category_means, by = "category_name_3") %>% 
  mutate(residuals = target - mean_target) %>% 
  `$`("residuals")
mean(train_set_residuals^2)
```

Validation set error.

```{r}
val_set_residuals = train_val %>% 
  left_join(category_means, by = "category_name_3") %>% 
  mutate(residuals = target - mean_target) %>% 
  `$`("residuals")
mean(val_set_residuals^2)
```

Not too different.